# -*- coding: utf-8 -*-
"""Untitled20.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xw_ybjKj0OVXNdJJXoh9VYQ2R1rShkWT
"""

#!pip install --upgrade tensorflow==2.5.0
#!pip install --upgrade tensorflow-gpu==2.5.0
#!pip install tensorlayer
#!pip install --upgrade tensorflow_privacy==0.5.2
#!pip install tools
from collections import deque
import numpy as np
import time
import argparse
import matplotlib.pyplot as plt
import tensorflow as tf
import numpy.random as rn
import argparse
from mdp import MountainCarEnv
from helper import get_alphas, get_mean_values, get_norm_alphas

parser = argparse.ArgumentParser()
parser.add_argument('--algo', type=str, default='SGD')
parser.add_argument('--optim', type=str, default='SGD')
parser.add_argument('--activation', type=str, default='relu')
parser.add_argument('--with_privacy', type=str, default='true')
config = parser.parse_args()

optimizer = config.optim
algorithm = config.algo

if algorithm == "dqn":
    dp_flag = 0
    sigma = 3.04
else:
    if config.with_privacy == 'true':
        dp_flag = 1
        sigma = 151
    else:
        dp_flag = 0
        sigma = 10000

epsilons = [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]
sigmas = [151, 22.5, 9.8, 5.35, 3.04, 1.54, 0.989]
l2_norm_clip = 1.0
noise_multiplier = 0.001
microbatches = 4

reward_classes = [0, 1, 2]
d = 10
stdev = 0.2/3
means = [-1.2, -1.0, -0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6]
num_policies = 2 # total - 2 {random, optimal}
num_trajs = 2 # = m
k = 1

if algorithm == "dqn":
    from train_dqn import MountainCarTrain
else:
    from train_ppo import MountainCarTrain
    
for reward_class in reward_classes:
    print(f"Reward Class = {reward_class}")
    env = MountainCarEnv(goal_velocity=0, reward_class=reward_class)

    overall_start = time.time()

    ground_alphas_norm = get_norm_alphas(reward_class)

    alpha_list, alphas_norm, alpha_dists, ground_rs, rs, dists = [[] for i in range(6)]

    start = time.time()

    opt_model = MountainCarTrain(env=env, reward_type="inbuilt", alphas=None, std=None, means=None, sigma=sigma, optimizer=optimizer, dp_flag=dp_flag) # train optimal policy
    opt_model.start()
    end = time.time()
    time_taken = end - start

    # start with creating trajectories from random and optimal policy: policy number 1 and 5 
    V_star, ground_rs = get_mean_values(opt_model, num_trajs, d=d, std=stdev, gamma=opt_model.gamma, mode="optimal")

    V_vecs = []
    v_temp, random_rews = get_mean_values(opt_model, num_trajs, d=d, std=stdev, gamma=opt_model.gamma, mode="random")
    V_vecs.append(v_temp)
    rs.append(random_rews)

    dist_temp = []
    r_dist = abs(random_rews - ground_rs) # because it is a single value
    dist_temp.append(r_dist)
    if ground_rs * random_rews < 0:
        sign_ch = 1 
    else:
        sign_ch = 0
    dist_temp.append(sign_ch)
    dists.append(dist_temp)

    curr_alphas = get_alphas(k=k, d=d, V_star=V_star, V_vecs=V_vecs)
    alpha_list.append(curr_alphas)

    norms = []
    norms.append(curr_alphas/ np.linalg.norm(curr_alphas, ord=1))
    norms.append(curr_alphas/ np.linalg.norm(curr_alphas, ord=2))
    norms.append(curr_alphas/ np.linalg.norm(curr_alphas, ord=np.inf))
    alphas_norm.append(norms)

    ds = []
    ds.append(np.linalg.norm(norms[0] - ground_alphas_norm[0], ord=1))
    ds.append(np.linalg.norm(norms[1] - ground_alphas_norm[1], ord=2))
    ds.append(np.linalg.norm(norms[2] - ground_alphas_norm[2], ord=np.inf))
    sign_ch_count = 0
    for j in range(len(curr_alphas)):
        if curr_alphas[j] * ground_alphas_norm[1][j] < 0:
            sign_ch_count += 1
    ds.append(sign_ch_count)
    alpha_dists.append(ds)


    for loop in range(num_policies):

        k += 1

        start = time.time()
        curr_model = MountainCarTrain(env=env, reward_type="custom", alphas=curr_alphas, std=stdev, means=means, sigma=sigma, optimizer=optimizer, dp_flag=dp_flag)
        curr_model.start()
        end = time.time()
        time_taken = end - start

        curr_V, curr_rews = get_mean_values(curr_model, num_trajs, d=d, std=stdev, gamma=curr_model.gamma, mode="optimal")
        V_vecs.append(curr_V)
        rs.append(curr_rews)

        curr_alphas = get_alphas(k=k, d=d, V_star=V_star, V_vecs=V_vecs)
        alpha_list.append(curr_alphas)

        dist_temp = []
        r_dist = abs(curr_rews - ground_rs) 
        dist_temp.append(r_dist)
        if ground_rs * random_rews < 0:
            sign_ch = 1 
        else:
            sign_ch = 0

        dist_temp.append(sign_ch)
        dists.append(dist_temp)

        norms = []
        norms.append(curr_alphas/ np.linalg.norm(curr_alphas, ord=1))
        norms.append(curr_alphas/ np.linalg.norm(curr_alphas, ord=2))
        norms.append(curr_alphas/ np.linalg.norm(curr_alphas, ord=np.inf))
        alphas_norm.append(norms)

        ds = []
        ds.append(np.linalg.norm(norms[0] - ground_alphas_norm[0], ord=1))
        ds.append(np.linalg.norm(norms[1] - ground_alphas_norm[1], ord=2))
        ds.append(np.linalg.norm(norms[2] - ground_alphas_norm[2], ord=np.inf))
        sign_ch_count = 0
        for j in range(len(curr_alphas)):
            if curr_alphas[j] * ground_alphas_norm[1][j] < 0:
                sign_ch_count += 1
        ds.append(sign_ch_count)
        alpha_dists.append(ds)

    print("Utility         : ", ground_rs)
    print("Reward distances: ", dists[-1])
    print("Alpha  distances: ", alpha_dists[-1])
    overall_end = time.time()
    print("Overall time: ", overall_end - overall_start)

